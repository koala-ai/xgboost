# 主流程

&emsp;&emsp;整个训练过程的主要流程如下：

```
cli_main.cc:
main()
     -> CLIRunTask()
          -> CLITrain()
               -> DMatrix::Load()
               -> learner = Learner::Create()
               -> learner->Configure()
               -> learner->InitModel()
               -> for (i = 0; i < param.num_round; ++i)
                    -> learner->UpdateOneIter()
                    -> learner->Save()    
learner.cc:
Create()
      -> new LearnerImpl() //派生自learner
Configure() // 配置obj, updater和模型的相关参数
InitModel()
     -> LazyInitModel()
          -> obj_ = ObjFunction::Create()
               -> objective.cc
                    Create()
                         -> SoftmaxMultiClassObj(multiclass_obj.cc)/
                            LambdaRankObj(rank_obj.cc)/
                            RegLossObj(regression_obj.cc)/
                            PoissonRegression(regression_obj.cc)
          -> gbm_ = GradientBooster::Create()
               -> gbm.cc
                    Create()
                         -> GBTree(gbtree.cc)/
                            GBLinear(gblinear.cc)
          -> obj_->Configure()
          -> gbm_->Configure()
UpdateOneIter()
      -> PredictRaw() // 预测样本标签
      -> obj_->GetGradient() // 计算样本的一阶导，二阶导
      -> gbm_->DoBoost()  // 进行boost(tree model/linear model)        

gbtree.cc:
Configure() // 根据配置初始化树相关操作
      -> for (up in updaters)
           -> up->Init()
DoBoost() // 每一级生成一颗RegTree
      -> BoostNewTrees()
           -> new_tree = new RegTree()
           -> for (up in updaters)
                -> up->Update(new_tree)    

tree_updater.cc:
Create()
     -> ColMaker/DistColMaker(updater_colmaker.cc)/
        SketchMaker(updater_skmaker.cc)/
        TreeRefresher(updater_refresh.cc)/
        TreePruner(updater_prune.cc)/
        HistMaker/CQHistMaker/
                  GlobalProposalHistMaker/
                  QuantileHistMaker(updater_histmaker.cc)/
        TreeSyncher(updater_sync.cc)
```

&emsp;&emsp;从上面的代码主流程可以看到，在XGBoost的实现中，对算法进行了模块化的拆解，几个重要的部分分别是：

- ObjFunction：对应于不同的Loss Function，可以完成一阶和二阶导数的计算。损失函数的介绍见[损失函数](损失函数.md)
- GradientBooster：用于管理Boost方法生成的Model，这里的Booster Model既可以对应于线性Booster Model，也可以对应于Tree Booster Model。具体信息见[GradientBooster](GradientBooster.md)
- Updater：用于建树，根据具体的建树策略不同，也会有多种Updater。具体信息见[updater](updater.md)